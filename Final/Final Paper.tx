\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{fancyhdr}
\usepackage[letterpaper, margin=.8in]{geometry}
\usepackage{sidecap}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines

\center 
\textsc{\LARGE Marist College}\\[1.5cm]
\textsc{\Large ST: Deep Learning with TensorFlow}\\[0.5cm] 
\textsc{\large CMPT 469 Final Project}\\[0.5cm] 

\HRule \\[0.4cm]
{ \huge \bfseries Artistic Style Transfer with TensorFlow}\\[0.4cm]
\HRule \\[1.5cm]
 
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Authors:}\\
Brian \textsc{Henderson} % Your name
\\
Sami \textsc{Ellougani} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Professor:} \\
Dr. Pablo \textsc{Rivas} % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

{\large \today}\\[2cm]

\begin{center}
  \includegraphics[height=50mm]{tf.png}
\end{center}


\end{titlepage}

\begin{abstract}
 "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
\end{abstract}

\section{Introduction}
This semester, we took on the challenge of learning how Deep Neural Networks work by delving deep into the implementation of an Artistic Style Transfer. An Artistic Style Transfer takes the style of one photo,
and puts it over another photo with the same artistic style. 

\begin{center}
  \includegraphics[height=75mm]{milestone_example.png}
\end{center}

Humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image.[2]
This seems like an almost impossible task to take a style from one photo to another, but to achieve this we will be using convolution neural networks to train styles. 
We've decided to apply the style transfer in real time, which will involve training networks with a designated style. We will use several resources as guides on how to begin TensorFlow implementation. Our objective is letting a user have the ability to upload upload various influential, abstract, and unique styles from different historical and modern time periods, select a preloaded style, and produce the output.


\section{Leon A. Gatys  A Neural Algorithm of Artistic Style}
\subsection{Interpretation of Convolutional Neural Networks}
Convolutional Neural Networks provide the best solution to handle image processing problems. Convolutional Neural Networks have several layers that process visual data in a hierachical format.
This can then be helpful in retrieving data regarding what type of Artistic Style an image contains. Gatys goes into more depth about what a Convolution Neural Network consists of.
"Convolutional Neural Networks consist of layers of
small computational units that process visual information hierarchically in a feed-forward manner.
 Each layer of units can be understood as a collection of image filters, each of which
extracts a certain feature from the input image. Thus, the output of a given layer consists of
so-called feature maps: differently filtered versions of the input image" \cite{Gatys}

\subsection{Use of Convolution Neural Networks}
Very simply put, Convolution Neural Networks are useful for processing image data, which then helps us in the challenge of object recognition. 
Gatys states his reasoning for choosing a Convolution Neural Network for solving the issue of retrieving Artistic Style from a photograph. 
"When Convolutional Neural Networks are trained on object recognition, they develop a
representation of the image that makes object information increasingly explicit along the processing
hierarchy. Therefore, along the processing hierarchy of the network, the input image
is transformed into representations that increasingly care about the actual content of the image
compared to its detailed pixel values. We can directly visualise the information each layer
contains about the input image by reconstructing the image only from the feature maps in that
layer" \cite{Gatys} \\ \\
As we process the input image, we are able to get pixel values which we can then use to have a data representation of the Artistic Style the photograph has.
This then helps us towards our end goal of masking a secondary photo with the initial photo's Artistic Style. 

\subsection{Representation of Style}
Through this algorithm, we gain a representation of the style of an image
by including a feature that was originally used to capturing the texture of the photo. 
"By including the feature correlations of multiple layers, we obtain a stationary, multi-scale representation of the input
image, which captures its texture information but not the global arrangement." \cite{Gatys}

\subsection{Figure of Convolution Neural Network}
\includegraphics[height=75mm, width=160mm]{cnn.png}

%%%%%%%%%%%%

\section{Justin Johnson's Perceptual Losses for Real-Time Style Transfer and Super-Resolution}
Justin Johnson, \textit{Department of CS, Stanford University}, describes image transformation as a problem, "Where input image is transformed into an output image." \cite{Johnson} Recent and common solutions for this problem involve training feed-forward convoloutional neural networks, that produces the image by using a per-pixel loss from the output and original image. Successful images have also been generated by optimizing and defining perceptual loss functions that extract high-level functions from the pre-trained networks. Johnson introduces a hybrid concept of both approaches, suggesting the use of perceptual loss functions for training feed-forward networks as a solution for the image transformation problem. 

\subsection{Per-Pixel Feed-Forward and Perceptual Loss Functions}
Per-pixel feed forward functions are very effective and efficient, they produce the output image by differentiating the pixels between the ground truth image and the output image. However it does not fully capture perceptual differences. When comparing two identical images, one image offset by one pixel, although being perceptually analogous, they would be perceived very differently from a per-pixel function. Perceptual loss functions are very effective at producing high quality images, however they are also very time and resource consuming. Perceptual loss functions extract the high level features from pre-trained convolutional neural networks. The output image is produced from optimizing a loss function, which produces high quality images, however it takes much longer since it must also solve the optimization problem.

\subsection{Johnson's Hybrid Approach}
Both of the explained approaches to solving the image transformation problem have their strengths and weakness's, so Johnson proposes taking the strengths from both and combine them together. The networks are trained using perceptual loss functions that are dependant on the high-level features that have been trained in a loss network. The training is done in a feed-forward transformation network. This hybrid allows for network to be executed in real time, and distinguishes image similarities on a broader spectrum.

\subsection{Methodology Architecture}
Johnson's architecture is comprised of two fundamentals, the image transformation network and the loss function. The Image transformation network is a deep residual convoloutional neural network that is characterized by weights, and is trained using stochastic gradient decent that minimizes the weighted combination of loss functions. The loss network defines the feature reconstruction loss and style reconstruction loss, which measures the differences in the content and style between the images. The benefits of this methodology is that the convoloutional neural networks trained have already learned to encode the perceptual and semantic information that are measured in the loss functions. Johnson makes use of the pre-trained network as a fixed loss network to define the loss functions. The deep convoloutional transformation network is trained by the loss functions, which are also deep convoloutional networks. \cite{Johnson}


%%%%%%%%%%%%

\section{Dmitry Ulyanov’s Instance Normalization: The Missing Ingredient for Fast Stylization}

\subsection{One Small Change, Significant Improvement}
Dmitry Ulyanov and his team recognizes how a minor change in the stylization architecture in image transformation can result in significant qualitative improvements in the output image. They build upon the feed-forward stylization architecture by replacing the batch normalization  layers with instance normalization layers in the generator architecture. It is also kept at test time instead of freezing and simplifying them out as done in batch normalization. This process allows the normalization any instance specific contrast information, overall simplifying the generation of the output image and producing higher quality results.

\subsection{Trends and Observations}
Some trends and inferences that were observed in the experiments were that the network that was trained with only $16$ images produced higher quality results than the network trained with thousands of images. They ultimately found that the networks that were trained with smaller batches of images and stopped early in the training process produced better results. It was also observed and mentioned how the stylization should not be dependent on the contrast of the content image, since the output image's contrast should be similar to the content image.
\\\\
By replacing the batch normalization with the instance normalization and applying at test time, the results are significantly more impressive. What this does is it prevents the instance specific mean and covariance shift, resulting in a simpler learning process. 


%%%%%%%%%%%%


\section{Pretrained VGG-19 Model}
\subsection{Visual Geometry Group}
The Visual Geometry Group is an academic group that focuses on computer vision at Oxford University. The Visual Geometry Group has several publications and set the bar high in the field
of visual recognition. They contribute in several areas of evaluation of Convolutional Neural Networks to increase improvement of image recognition.

\subsection{VGG-16/19 Model}
VGG-16/19 is a pretrained model that has a rich feature representation for a wide variety of images. VGG-16/19 is pretrained using the ImageNet database, which means that the model is trained on more than a million images. As a result, the model excels in object recognition for a wide variety of images. The difference between VGG-16/19 is the amount of layers that both models use. VGG-16 uses 16 layers, while VGG-19 uses 19 layers.

\subsection{ImageNet Database}
ImageNet is a current research effort to provide researchers and engineers the capability to access an image database. This image database is vast as it contains more than a million images.
ImageNet is organized according to the WordNet hierarchy. The images in ImageNet are categorized by a couple of words, called "synonym sets." There are more than 100,000 synonym sets in 
the WordNet database, and the majority of them are nouns. ImageNet provides on average 1000 images to illustrate each synonym set.


\subsection{Figure of VGG-19 Model}
\begin{center}
  \includegraphics[height=75mm]{vgg-19.jpg}
\end{center}

\section{Microsoft Coco Dataset}

\subsection{About}
The Microsoft Coco dataset contains over 330,000 images, and more than 200,000 of them being labeled. Item labeling is used for instance segmentation to aid object recognition. 

\subsection{Purpose of Coco Dataset}
This dataset is used as well as the ImageNet database in our VGG-19 model for statisical analysis. "We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."\cite{Coco}

\subsection{Figure of Annotation Pipeline}
\includegraphics[height=75mm, width=160mm]{pipeline.png}

\section{Experiments}
Using Francis Picabia's 1913 painting, \textit{Udnie}, we ran several experiments transferring the style onto a picture of Marist College's Hancock Center. The purpose of the individualized experiments were diverse, however the general goal we were looking to accomplish was to modify the weights and see the affect on the output image. The code used for these experiments was developed by Logan Engstrom, \textit{Massachusetts Institute of Technology}, using technologies and concepts mentioned in this paper. \cite{Lengstrom} We installed tensorflow from sources on a gCloud GPU instance, modified the weights, and analyzed the results.
\\
\\
*Experiments were run on Google’s gCloud Service using a NVIDIA Tesla K80 GPU
\\

\begin{minipage}[c]{0.3\textwidth}
\includegraphics[height=50mm]{udnie.jpg}
\end{minipage}
\begin{minipage}[c]{0.6\textwidth}
\includegraphics[height=50mm]{hancock.jpg}
\end{minipage}
\subsection{Experiment 00}
\subsubsection{Hypothesis} Get a base understanding of the weights and modify appropriately using the results of this experiment.
\subsubsection{Weights and Result}
\begin{minipage}[t]{0.4\textwidth}
\begin{tabular}{l l}
    \textbf{Epochs} &$3$     \\
    \textbf{Batch Size}     & $4$     \\
    \textbf{Content Weight} & $7.5e0$ \\
    \textbf{Learning Rate}  & $1e-3$  \\
    \textbf{Style Weight}   & $1e2$ \\
    \textbf{Completion Time}  & 24hrs  \
\vspace{3.5cm}\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.1\textwidth}
\end{minipage}
\begin{minipage}[t]{0.6\textwidth}
\includegraphics[height=50mm]{results/00.png}
\end{minipage} \vspace{-3cm}
\subsubsection{Analysis}
Overall, the first experiment was successful with the production of a quality image. The sky's portion of the image has abstract "clouds," giving the image a sharp look. Downside to this architecture is training time took a full 24 hours to complete.


\subsection{Experiment 01}
\subsubsection{Hypothesis} Attempt to decrease training time with one less epoch, but increased batch size. Also increase the content weight and learning rate at an attempt to have the output image a little more smoother.
\subsubsection{Weights and Result}
\begin{minipage}[t]{0.4\textwidth}
\begin{tabular}{l l}
    \textbf{Epochs} & $2$     \\
    \textbf{Batch Size}     & $20$     \\
    \textbf{Content Weight} & $1.5e1$ \\
    \textbf{Learning Rate}  & $1e-2$  \\
    \textbf{Style Weight}   & $1e2$ \\
    \textbf{Completion Time}  & 12hrs  \
\vspace{3.5cm}\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.1\textwidth}
\end{minipage}
\begin{minipage}[t]{0.6\textwidth}
\includegraphics[height=50mm]{results/01.png}
\end{minipage} \vspace{-3cm}
\subsubsection{Analysis}
The training time for this network was 50\% less than the previous experiment and still produced a quality image. The style did smooth out as predicted, as seen in the sky.

\subsection{Experiment 02}
\subsubsection{Hypothesis} Try something different by increasing the style weight significantly and see what the result is.
\subsubsection{Weights and Result}
\begin{minipage}[t]{0.4\textwidth}
\begin{tabular}{l l}
    \textbf{Epochs}          & $2$     \\
    \textbf{Batch Size}      & $20$    \\
    \textbf{Content Weight}  & $1.5e1$ \\
    \textbf{Learning Rate}   & $1e-2$  \\
    \textbf{Style Weight}    & $1e4$   \\
    \textbf{Completion Time} & 12hrs \
\vspace{3.5cm}\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.1\textwidth}
\end{minipage}
\begin{minipage}[t]{0.6\textwidth}
\includegraphics[height=50mm]{results/02.png}
\end{minipage} \vspace{-3cm}
\subsubsection{Analysis}
Not a successful experiment in respect to the quality of the output image, but definitely an interesting result.


\subsection{Experiment 03}
\subsubsection{Hypothesis} Revert the style weight back, and lower the content weight from experiment 01.
\subsubsection{Weights and Result}
\begin{minipage}[t]{0.4\textwidth}
\begin{tabular}{l l}
    \textbf{Epochs}          & $2$     \\
    \textbf{Batch Size}      & $20$    \\
    \textbf{Content Weight}  & $6e0$ \\
    \textbf{Learning Rate}   & $1e-2$  \\
    \textbf{Style Weight}    & $1e2$   \\
    \textbf{Completion Time} & 12hrs \
\vspace{3.5cm}\\
\end{tabular}
\end{minipage}
\begin{minipage}{0.1\textwidth}
\end{minipage}
\begin{minipage}[t]{0.6\textwidth}
\includegraphics[height=50mm]{results/03.png}
\end{minipage} \vspace{-3cm}
\subsubsection{Analysis}
Successful experiment. Interesting in regards to the white glowing outline towards the top and left of the building.

\section{Conclusion}




\medskip
\begin{thebibliography}{9}
\bibitem{Gatys} 
Gatys, Leon A., et al.
\textit{ “A Neural Algorithm of Artistic Style.”}.  [1508.06576] A Neural Algorithm of Artistic Style, 2 Sept. 2015, arxiv.org/abs/1508.06576.\

\bibitem{Coco} 
Lin, Tsung-Yi, et al. 
\textit{ “Microsoft COCO: Common Objects in Context.” }.  [1405.0312] Microsoft COCO: Common Objects in Context, 21 Feb. 2015, arxiv.org/abs/1405.0312.

\bibitem{Johnson}
Justin Johnson

\bibitem{Lengstrom}
git hub repo

\end{thebibliography}

\end{document}


\end{document}   